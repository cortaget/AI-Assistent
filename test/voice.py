from TTS.api import TTS
import torch
import numpy as np
import sounddevice as sd
import os
import time
import queue
import threading

print("=" * 60)
print("–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô XTTS –ë–ï–ó –ê–†–¢–ï–§–ê–ö–¢–û–í")
print("=" * 60)


# 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
def get_model_path():
    if os.name == 'nt':
        username = os.getenv('USERNAME')
        base_path = f"C:\\Users\\{username}\\AppData\\Local\\tts"
    else:
        base_path = os.path.expanduser("~/.local/share/tts")

    return os.path.join(base_path, "tts_models--multilingual--multi-dataset--xtts_v2")


# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nüñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ XTTS-v2...")
try:
    from TTS.tts.configs.xtts_config import XttsConfig
    from TTS.tts.models.xtts import Xtts

    model_path = get_model_path()
    config_path = os.path.join(model_path, "config.json")

    if not os.path.exists(config_path):
        print("‚ö†Ô∏è –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫, –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        tts_api = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
        model_path = get_model_path()

    config = XttsConfig()
    config.load_json(config_path)
    model = Xtts.init_from_config(config)
    model.load_checkpoint(config, checkpoint_dir=model_path, eval=True)
    model.to(device)

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    exit()

# 3. –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–π –≥–æ–ª–æ—Å
reference_audio = r"..\voices\my_voice_reference.wav"
if not os.path.exists(reference_audio):
    print(f"‚ùå –§–∞–π–ª '{reference_audio}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    exit()

print(f"‚úÖ –†–µ—Ñ–µ—Ä–µ–Ω—Å: {reference_audio}")

# 4. –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–æ–≤
print("\nüîÑ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(
    audio_path=[reference_audio]
)
print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≥–æ—Ç–æ–≤—ã!")

# 5. ‚úÖ –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –∞—É–¥–∏–æ-—Å—Ç—Ä–∏–º —Å –æ—á–µ—Ä–µ–¥—å—é
audio_queue = queue.Queue()
stream_active = threading.Event()


def audio_callback(outdata, frames, time_info, status):
    """Callback –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –±–µ–∑ –ø–∞—É–∑"""
    if status:
        print(f"‚ö†Ô∏è –°—Ç–∞—Ç—É—Å: {status}")

    try:
        # –ë–µ—Ä—ë–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        data = audio_queue.get_nowait()

        if len(data) < len(outdata):
            # –î–æ–ø–æ–ª–Ω—è–µ–º —Ç–∏—à–∏–Ω–æ–π –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ
            outdata[:len(data)] = data.reshape(-1, 1)
            outdata[len(data):] = 0
        else:
            outdata[:] = data[:len(outdata)].reshape(-1, 1)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å
            if len(data) > len(outdata):
                audio_queue.put(data[len(outdata):])

    except queue.Empty:
        # –¢–∏—à–∏–Ω–∞ –µ—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞
        outdata.fill(0)


# –°–æ–∑–¥–∞—ë–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π output stream
output_stream = sd.OutputStream(
    samplerate=24000,
    channels=1,
    callback=audio_callback,
    blocksize=2048  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞
)


def speak_smooth(text):
    """–£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ó–í–£–ß–ö–ê: –ü–ª–∞–≤–Ω–∞—è –±–µ–∑ —Ä–∞–∑—Ä—ã–≤–æ–≤"""
    print(f"\nüéôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '{text[:50]}...'")
    t0 = time.time()

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
        chunks = model.inference_stream(
            text,
            "ru",
            gpt_cond_latent,
            speaker_embedding,
            stream_chunk_size=20,  # –ë–æ–ª—å—à–µ —á–∞–Ω–∫ = –º–µ–Ω—å—à–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            enable_text_splitting=True
        )

        # –ó–∞–ø—É—Å–∫–∞–µ–º stream –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–ø—É—â–µ–Ω
        if not output_stream.active:
            output_stream.start()

        first = True
        total_samples = 0

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤ –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
        all_audio = []

        for i, chunk in enumerate(chunks):
            if first:
                print(f"‚ö° –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ –∑–∞: {(time.time() - t0) * 1000:.0f}–º—Å")
                first = False

            chunk_array = chunk.squeeze().cpu().numpy()
            all_audio.append(chunk_array)
            total_samples += len(chunk_array)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω –º–∞—Å—Å–∏–≤
        full_audio = np.concatenate(all_audio)

        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {total_samples} —Å—ç–º–ø–ª–æ–≤ –∑–∞ {(time.time() - t0):.2f}—Å–µ–∫")
        print("üîä –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ...")

        # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º –æ–¥–Ω–∏–º –∫—É—Å–∫–æ–º (–ë–ï–ó –ø–∞—É–∑!)
        sd.play(full_audio, 24000)
        sd.wait()

        print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: {(time.time() - t0):.2f}—Å–µ–∫")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


# 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
print("\n" + "=" * 60)
print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–õ–ê–í–ù–û–ô –û–ó–í–£–ß–ö–ò")
print("=" * 60)

test_phrases = [
    "–ü—Ä–∏–≤–µ—Ç! –¢–µ–ø–µ—Ä—å –∑–≤—É–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–ª–∞–≤–Ω—ã–º –∏ –±–µ–∑ —Ä–∞–∑—Ä—ã–≤–æ–≤.",
    "–Ø –∏—Å–ø–æ–ª—å–∑—É—é —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–≤—É–∫–∞.",
    "–§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–∑–≤—É—á–∫–∏ –∏ –ø–ª–∞–≤–Ω–æ—Å—Ç–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è."
]

for i, phrase in enumerate(test_phrases, 1):
    print(f"\n--- –¢–ï–°–¢ {i} –∏–∑ {len(test_phrases)} ---")
    speak_smooth(phrase)

    if i < len(test_phrases):
        print("\n‚è∏Ô∏è –ü–∞—É–∑–∞ 2 —Å–µ–∫—É–Ω–¥—ã...")
        time.sleep(2)

print("\n" + "=" * 60)
print("üéâ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
print("=" * 60)

# –û—á–∏—Å—Ç–∫–∞
if output_stream.active:
    output_stream.stop()
output_stream.close()
