# voice_assistant.py - –ß–ò–°–¢–ê–Ø –í–ï–†–°–ò–Ø
import json
import requests
import pyttsx3
import pyaudio
from vosk import Model, KaldiRecognizer
from core.plugin_loader import load_plugins
import threading
import queue
from memory_manager import MemoryManager

LLM_URL = "http://127.0.0.1:11434/api/generate"
LLM_MODEL = "gemma3:4b"

memory = MemoryManager()
memory.debug_mode = False  # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏

model = Model("E:\\python\\PYCHARM\\UZISpeach\\vosk-model-small-ru-0.22")
recognizer = KaldiRecognizer(model, 16000)

p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=16000,
                input=True,
                frames_per_buffer=4096)
stream.start_stream()

speech_queue = queue.Queue()


def speech_worker():
    tts = pyttsx3.init()
    voices = tts.getProperty('voices')
    for voice in voices:
        if "irina" in voice.name.lower():
            tts.setProperty('voice', voice.id)
            break
    tts.setProperty('rate', 160)

    while True:
        text = speech_queue.get()
        if text is None:
            break
        tts.say(text)
        tts.runAndWait()
        speech_queue.task_done()


speech_thread = threading.Thread(target=speech_worker, daemon=True)
speech_thread.start()


def speak(text):
    speech_queue.put(text)


def listen_command():
    print("üéôÔ∏è –ì–æ–≤–æ—Ä–∏...")
    try:
        stream.read(stream.get_read_available(), exception_on_overflow=False)
    except:
        pass

    while True:
        data = stream.read(4096, exception_on_overflow=False)
        if recognizer.AcceptWaveform(data):
            result = json.loads(recognizer.Result())
            text = result.get("text", "")
            if text:
                print(f"üìù {text}")
                return text


def auto_save_memory(user_input: str, assistant_reply: str):
    """–ò–ò —Å–∞–º–æ —Ä–µ—à–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–ª–∏ –Ω–µ—Ç"""

    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è?
    check_prompt = f"""–î–∏–∞–ª–æ–≥:
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}
–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {assistant_reply}

–°–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–∞–∂–Ω—É—é –ª–∏—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è (–∏–º—è, —Ä–∞–±–æ—Ç–∞, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è, —É–≤–ª–µ—á–µ–Ω–∏—è, –ø—Ä–∞–≤–∏–ª–∞)?

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û "–î–ê" –∏–ª–∏ "–ù–ï–¢"."""

    payload = {
        "model": LLM_MODEL,
        "prompt": check_prompt,
        "stream": False,
        "options": {"temperature": 0.1}
    }

    try:
        response = requests.post(LLM_URL, json=payload, timeout=10)
        if response.status_code == 200:
            decision = response.json().get("response", "").strip().upper()

            if "–î–ê" in decision:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç
                extract_prompt = f"""–ò–∑ —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑–≤–ª–µ–∫–∏ –û–î–ò–ù –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ–∞–∫—Ç –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ:

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_input}

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å [–≥–ª–∞–≥–æ–ª] [–æ–±—ä–µ–∫—Ç]"
–ü—Ä–∏–º–µ—Ä: "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ª—é–±–∏—Ç –ª–æ—à–∞–¥–µ–π"

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç–æ–º –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."""

                payload2 = {
                    "model": LLM_MODEL,
                    "prompt": extract_prompt,
                    "stream": False,
                    "options": {"temperature": 0.1}
                }

                response2 = requests.post(LLM_URL, json=payload2, timeout=10)
                if response2.status_code == 200:
                    fact = response2.json().get("response", "").strip()

                    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –ª–∏—à–Ω–µ–µ
                    fact = fact.strip('"\'').strip()

                    if len(fact) > 10 and "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª" in fact.lower():
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–ø—Ä—è–º—É—é
                        memory.add_memory(fact, memory_type="user_info")
                        print(f"üíæ {fact}")

    except Exception as e:
        pass  # –ú–æ–ª—á–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏


def query_llm_stream(user_input):
    """–ó–∞–ø—Ä–æ—Å —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º"""

    # –ü–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏
    relevant_memories = memory.search_memory(user_input, top_k=2)

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏
    memory_context = ""
    if relevant_memories:
        filtered = [m for m in relevant_memories if m['relevance'] > 0.4]
        if filtered:
            memory_context = "[–í–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ø–∞–º—è—Ç–∏]:\n"
            for mem in filtered:
                memory_context += f"- {mem['content']}\n"
            memory_context += "\n"

    # –ü—Ä–æ–º–ø—Ç
    full_prompt = memory_context + f"User: {user_input}\nAssistant:"

    payload = {
        "model": LLM_MODEL,
        "prompt": full_prompt,
        "stream": True
    }

    try:
        response = requests.post(LLM_URL, json=payload, stream=True)

        if response.status_code != 200:
            return f"–û—à–∏–±–∫–∞: {response.status_code}"

        reply = ""
        print("üí¨ ", end='', flush=True)

        for line in response.iter_lines():
            if line:
                part = json.loads(line.decode('utf-8')).get("response", "")
                print(part, end='', flush=True)
                reply += part

        print()

        # ‚úÖ –ò–ò —Å–∞–º–æ —Ä–µ—à–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–ª–∏ –Ω–µ—Ç
        auto_save_memory(user_input, reply)

        return reply

    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {str(e)}"


def handle_memory_commands(user_input: str) -> bool:
    """–ö–æ–º–∞–Ω–¥—ã –ø–∞–º—è—Ç–∏"""
    lower_input = user_input.lower()

    if "–∑–∞–ø–æ–º–Ω–∏" in lower_input:
        content = user_input.split("–∑–∞–ø–æ–º–Ω–∏", 1)[-1].strip()
        if content:
            memory.add_memory(content, memory_type="user_info")
            speak("–ó–∞–ø–æ–º–Ω–∏–ª")
            return True

    if "—á—Ç–æ —Ç—ã –ø–æ–º–Ω–∏—à—å" in lower_input or "–ø–æ–∫–∞–∂–∏ –ø–∞–º—è—Ç—å" in lower_input:
        memories = memory.list_all_memories()
        if memories:
            print(f"\nüíæ –ü–∞–º—è—Ç—å ({len(memories)} –∑–∞–ø–∏—Å–µ–π):")
            for i, mem in enumerate(memories, 1):
                print(f"{i}. {mem['content']}")
            speak(f"–Ø –ø–æ–º–Ω—é {len(memories)} –∑–∞–ø–∏—Å–µ–π")
        else:
            speak("–ü–∞–º—è—Ç—å –ø—É—Å—Ç–∞")
        return True

    if "–æ—á–∏—Å—Ç–∏ –ø–∞–º—è—Ç—å" in lower_input:
        memory.clear_all_memories()
        speak("–ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
        return True

    return False


def main():
    plugin_handlers = load_plugins()

    speak("–ü—Ä–∏–≤–µ—Ç")

    while True:
        user_input = listen_command()
        if not user_input:
            continue

        if any(word in user_input for word in ["–≤—ã—Ö–æ–¥", "—Å—Ç–æ–ø", "–≤—ã–∫–ª—é—á–∏—Å—å"]):
            speak("–ü–æ–∫–∞")
            speech_queue.put(None)
            speech_thread.join(timeout=2)
            break

        #–ø–æ–∫–∞ –Ω–µ –Ω–∞–¥–æ —ç—Ç–æ —Ç—Ä–æ–≥–∞—Ç—å
        #if handle_memory_commands(user_input):
        #    continue

        handled = False
        for handler in plugin_handlers:
            result = handler(user_input)
            if result:
                speak(result)
                handled = True
                break

        if not handled:
            reply = query_llm_stream(user_input)
            speak(reply)


if __name__ == "__main__":
    main()