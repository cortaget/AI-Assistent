# core/llm_client.py
import json
import requests
from typing import Dict, Any, Optional
from config import Config


class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM (Ollama)"""

    def __init__(self, config: Config = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∫–ª–∏–µ–Ω—Ç–∞

        Args:
            config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        self.config = config or Config()
        self.url = self.config.LLM_URL
        self.model = self.config.LLM_MODEL

    def quick_call(self, prompt: str, max_tokens: int = 50, temperature: float = None) -> str:
        """
        –ë—ã—Å—Ç—Ä—ã–π –≤—ã–∑–æ–≤ LLM –±–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞

        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Returns:
            –û—Ç–≤–µ—Ç LLM
        """
        if temperature is None:
            temperature = self.config.LLM_QUICK_CALL_TEMPERATURE

        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_k": self.config.LLM_QUICK_CALL_TOP_K,
                "top_p": self.config.LLM_QUICK_CALL_TOP_P
            }
        }

        try:
            response = requests.post(self.url, json=payload, timeout=100)
            if response.status_code == 200:
                return response.json().get("response", "").strip()
        except:
            pass

        return ""

    def stream_call(self, prompt: str, temperature: float = None) -> str:
        """
        –í—ã–∑–æ–≤ LLM —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º

        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Returns:
            –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç LLM
        """
        if temperature is None:
            temperature = self.config.LLM_TEMPERATURE

        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": temperature
            }
        }

        try:
            response = requests.post(self.url, json=payload, stream=True)
            if response.status_code != 200:
                return f"–û—à–∏–±–∫–∞: {response.status_code}"

            reply = ""
            print("üí¨ ", end='', flush=True)

            for line in response.iter_lines():
                if line:
                    part = json.loads(line.decode('utf-8')).get("response", "")
                    print(part, end='', flush=True)
                    reply += part

            print()
            return reply

        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {str(e)}"

    def non_stream_call(self, prompt: str, temperature: float = None, timeout: int = 100) -> str:
        """
        –í—ã–∑–æ–≤ LLM –±–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞

        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            –û—Ç–≤–µ—Ç LLM
        """
        if temperature is None:
            temperature = self.config.LLM_TEMPERATURE

        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": temperature}
        }

        try:
            response = requests.post(self.url, json=payload, timeout=timeout)
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            return ""
        except Exception as e:
            return f"–û—à–∏–±–∫–∞: {str(e)}"
